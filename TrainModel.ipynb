{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66176306-56ae-43e8-824d-c6108590c19b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn off Tensorflow warnings\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import pickle\n",
    "\n",
    "## Helper function\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def skipper(fname):\n",
    "    with open(fname) as fin:\n",
    "        no_comments = (line for line in fin if not line.lstrip().startswith('n'))\n",
    "        next(no_comments, None) # skip header\n",
    "        for row in no_comments:\n",
    "            yield row\n",
    "        \n",
    "# Load the data\n",
    "\n",
    "browse_data = np.loadtxt(skipper('Data/featurePCAP_Browsing.csv'), delimiter=',')\n",
    "chat_data = np.loadtxt(skipper('Data/featurePCAP_Chat.csv'), delimiter=',')\n",
    "mail_data = np.loadtxt(skipper('Data/featurePCAP_Mail.csv'), delimiter=',')\n",
    "p2p_data = np.loadtxt(skipper('Data/featurePCAP_P2P2.csv'), delimiter=',')\n",
    "raw_data = {'browse': browse_data, 'chat': chat_data, \n",
    "            'mail' : mail_data, 'p2p': p2p_data}\n",
    "\n",
    "# Standardize data\n",
    "total_data = np.concatenate([browse_data, chat_data, mail_data, p2p_data])\n",
    "scaler = StandardScaler().fit(total_data)\n",
    "scaled_data = dict()\n",
    "for data_type in raw_data:\n",
    "    scaled_data[data_type] = scaler.transform(raw_data[data_type])\n",
    "\n",
    "save_object(scaler, 'scaler.pkl')\n",
    "    \n",
    "del raw_data\n",
    "\n",
    "train_data, test_data = dict(), dict()\n",
    "train_labels, test_labels = dict(), dict()\n",
    "lstm_train_data, lstm_train_labels = dict(), dict()\n",
    "lstm_test_data, lstm_test_labels = dict(), dict()\n",
    "for index, data_type in enumerate(['browse','chat','mail','p2p']):\n",
    "    l = len(scaled_data[data_type])\n",
    "    train, test = scaled_data[data_type][:int(0.7*l)], scaled_data[data_type][int(0.7*l):]\n",
    "    if train.shape[0]%5 == 0:\n",
    "        split_train = np.array_split(train, train.shape[0]//5)\n",
    "    else:\n",
    "        split_train = np.array_split(train[:-(train.shape[0]%5)], train.shape[0]//5)\n",
    "    \n",
    "    if test.shape[0]%5 == 0:\n",
    "        split_test = np.array_split(test, test.shape[0]//5)\n",
    "    else:\n",
    "        split_test = np.array_split(test[:-(test.shape[0]%5)], test.shape[0]//5)\n",
    "    \n",
    "\n",
    "    lstm_train_data[data_type] = np.stack(split_train)\n",
    "    lstm_train_labels[data_type] = np.tile(np.eye(4)[index], (train.shape[0]//5, 1))\n",
    "    lstm_test_data[data_type] = np.stack(split_train)\n",
    "    lstm_test_labels[data_type] = np.tile(np.eye(4)[index], (train.shape[0]//5, 1))\n",
    "    train_data[data_type] = train\n",
    "    test_data[data_type] = test\n",
    "    train_labels[data_type] = np.tile(np.eye(4)[index], (train.shape[0], 1))\n",
    "    test_labels[data_type] = np.tile(np.eye(4)[index], (test.shape[0], 1))\n",
    "\n",
    "del scaled_data\n",
    "\n",
    "save_object(train_data, 'train_data.pkl')\n",
    "save_object(train_labels, 'train_labels.pkl')\n",
    "save_object(test_data, 'test_data.pkl')\n",
    "save_object(test_labels, 'test_labels.pkl')\n",
    "save_object(lstm_train_data, 'lstm_train_data.pkl')\n",
    "save_object(lstm_train_labels, 'lstm_train_labels.pkl')\n",
    "save_object(lstm_test_data, 'lstm_test_data.pkl')\n",
    "save_object(lstm_test_labels, 'lstm_test_labels.pkl')\n",
    "\n",
    "train_data = np.concatenate([train_data[d] for d in ['browse','chat','mail','p2p']])\n",
    "train_labels = np.concatenate([train_labels[d] for d in ['browse','chat','mail','p2p']])\n",
    "test_data = np.concatenate([test_data[d] for d in ['browse','chat','mail','p2p']])\n",
    "test_labels = np.concatenate([test_labels[d] for d in ['browse','chat','mail','p2p']])\n",
    "lstm_train_data = np.concatenate([lstm_train_data[d] for d in ['browse','chat','mail','p2p']])\n",
    "lstm_train_labels = np.concatenate([lstm_train_labels[d] for d in ['browse','chat','mail','p2p']])\n",
    "lstm_test_data = np.concatenate([lstm_test_data[d] for d in ['browse','chat','mail','p2p']])\n",
    "lstm_test_labels = np.concatenate([lstm_test_labels[d] for d in ['browse','chat','mail','p2p']])\n",
    "\n",
    "\n",
    "## Custom layer: used to map a function over a layer\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class MapLayer(k.layers.Layer):\n",
    "    def __init__(self, f, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.f = f\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.map_fn(self.f, inputs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['f'] = tf.keras.utils.serialize_keras_object(self.f)\n",
    "        return config\n",
    "\n",
    "## Small helper function used to compose layers\n",
    "\n",
    "def compose(layers, input_layer):\n",
    "    acc = input_layer\n",
    "    for layer in layers:\n",
    "        acc = layer(acc)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbac428-9e4b-4f0e-87cd-7d3c9aa81b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CNN SAE\n",
    "\n",
    "activation = k.activations.relu\n",
    "regularizer = k.regularizers.L1(l1=0.01)\n",
    "\n",
    "cnn_input = k.Input(shape=6)\n",
    "cnn_reshape = k.layers.Reshape((3, 2, 1))(cnn_input)\n",
    "cnn_conv_e_1 = k.layers.Conv2D(4, kernel_size=(2,1), activation=activation, padding='same')(cnn_reshape)\n",
    "cnn_conv_e_2 = k.layers.Conv2D(13, kernel_size=(2,1), activation=activation, activity_regularizer=regularizer, padding='same')(cnn_conv_e_1)\n",
    "cnn_conv_d_2 = k.layers.Conv2D(13, kernel_size=(2,1), activation=activation, padding='same')(cnn_conv_e_2)\n",
    "cnn_conv_d_1 = k.layers.Conv2D(4, kernel_size=(2,1), activation=activation, padding='same')(cnn_conv_d_2)\n",
    "cnn_output = k.layers.Dense(1, activation='sigmoid')(cnn_conv_d_1)\n",
    "cnn_flatten = k.layers.Flatten()(cnn_output)\n",
    "\n",
    "cnn_sae_flatten = k.layers.Flatten()(cnn_conv_e_2)\n",
    "cnn_sae_fcnn_1 = k.layers.Dense(64, activation=activation)(cnn_sae_flatten)\n",
    "cnn_sae_dropout_1 = k.layers.Dropout(rate=0.5)(cnn_sae_fcnn_1)\n",
    "cnn_sae_fcnn_2 = k.layers.Dense(24, activation=activation)(cnn_sae_dropout_1)\n",
    "cnn_sae_dropout_2 = k.layers.Dropout(rate=0.5)(cnn_sae_fcnn_2)\n",
    "cnn_sae_output = k.layers.Dense(4, activation='softmax')(cnn_sae_dropout_2)\n",
    "\n",
    "cnn_sae = k.Model(inputs=cnn_input, outputs=cnn_flatten, name='cnn_sae')\n",
    "cnn_sae.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "cnn_sae_encoder = k.Model(inputs=cnn_input, outputs=cnn_conv_e_2)\n",
    "cnn_sae_encoded_input = k.Input(shape=(3, 2, 13))\n",
    "_cnn_sae_decoder_layers = compose(cnn_sae.layers[-4:], cnn_sae_encoded_input)\n",
    "cnn_sae_decoder = k.Model(cnn_sae_encoded_input, _cnn_sae_decoder_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1e733-39a1-4e47-b16f-a33fa359a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_sae.fit(train_data, train_data, epochs=10, batch_size=32,\n",
    "           callbacks=[k.callbacks.ModelCheckpoint(filepath='cnn_sae.{epoch:02d}.keras'),\n",
    "                     k.callbacks.EarlyStopping(monitor='loss')])\n",
    "\n",
    "for layer in cnn_sae.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cbcb6-5c3a-4da5-99fb-4978dd5c8720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cnn_sae_full = k.Model(inputs=cnn_input, outputs=cnn_sae_output, name='cnn_sae_full')\n",
    "# cnn_sae_full.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "# cnn_sae_full.fit(train_data, train_labels, epochs=100, batch_size=32,\n",
    "#                callbacks=[k.callbacks.ModelCheckpoint(filepath='cnn_sae_full.{epoch:02d}.keras'),\n",
    "#                           k.callbacks.EarlyStopping(monitor='loss')])\n",
    "\n",
    "cnn_sae_full_scores = cnn_sae_full.evaluate(test_data, test_labels)\n",
    "print(\"\\n%s: %.2f%%\" % (cnn_sae_full.metrics_names[1], cnn_sae_full_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6de33a-209d-46b0-8153-ab8e5eeff41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM SAE\n",
    "\n",
    "lstm_input = tf.keras.layers.Input(shape=(5, 6))\n",
    "lstm_cnn_encoded_input_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Reshape((3, 2, 1)))(lstm_input)\n",
    "lstm_cnn_encoded_input_1 = tf.keras.layers.TimeDistributed(cnn_sae.layers[2])(lstm_cnn_encoded_input_0)\n",
    "lstm_cnn_encoded_input_2 = tf.keras.layers.TimeDistributed(cnn_sae.layers[3])(lstm_cnn_encoded_input_1)\n",
    "lstm_cnn_encoded_input_3 = tf.keras.layers.Reshape((5, 78))(lstm_cnn_encoded_input_2)\n",
    "lstm_e_1 = tf.keras.layers.LSTM(16, return_sequences=True)(lstm_cnn_encoded_input_3)\n",
    "lstm_e_2 = tf.keras.layers.LSTM(136, return_sequences=False, activity_regularizer=regularizer)(lstm_e_1)\n",
    "lstm_d_repeat = tf.keras.layers.RepeatVector(5)(lstm_e_2)\n",
    "lstm_d_1 = tf.keras.layers.LSTM(136, return_sequences=True)(lstm_d_repeat)\n",
    "lstm_d_2 = tf.keras.layers.LSTM(16, return_sequences=True)(lstm_d_1)\n",
    "lstm_output = k.layers.Dense(6, activation='sigmoid')(lstm_d_2)\n",
    "lstm_sae_fcnn_1 = tf.keras.layers.Dense(64, activation=activation)(lstm_e_2)\n",
    "lstm_sae_fcnn_2 = tf.keras.layers.Dense(24, activation=activation)(lstm_sae_fcnn_1)\n",
    "lstm_sae_output = tf.keras.layers.Dense(4, activation='softmax')(lstm_sae_fcnn_2)\n",
    "\n",
    "lstm_sae = tf.keras.Model(inputs=lstm_input, outputs=lstm_output, name='lstm_sae')\n",
    "lstm_sae.compile(loss='mse', optimizer='adam')\n",
    "lstm_sae_encoder = k.Model(lstm_input, lstm_e_2)\n",
    "\n",
    "# lstm_sae_full = k.Model(inputs=lstm_input, outputs=lstm_sae_output, name='lstm_sae_full')\n",
    "# lstm_sae_full.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0900656-c09b-474f-8bc3-15607468df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_sae.fit(lstm_train_data, lstm_train_data, epochs=10, batch_size=32,\n",
    "           callbacks=[k.callbacks.ModelCheckpoint(filepath='lstm_sae.{epoch:02d}.keras'),\n",
    "                       k.callbacks.EarlyStopping(monitor='loss')])\n",
    "\n",
    "for layer in lstm_sae.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c08e43-98d6-4b1e-bf2b-bc820a59c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_sae_full.fit(lstm_train_data, lstm_train_labels, epochs=100, batch_size=2048,\n",
    "#                   callbacks=[k.callbacks.ModelCheckpoint(filepath='lstm_sae_full.{epoch:02d}.keras'),\n",
    "#                              k.callbacks.EarlyStopping(monitor='loss')])\n",
    "# lstm_sae_full_scores = lstm_sae_full.evaluate(lstm_test_data, lstm_test_labels)\n",
    "# print(\"\\n%s: %.2f%%\" % (lstm_sae_full.metrics_names[1], lstm_sae_full_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6802f75-e33a-44ec-9769-b3cef73300b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked_cnn_lstm_sae_input = tf.keras.layers.Input(shape=(5, 6))\n",
    "stacked_cnn_lstm_sae_encoded_input_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Reshape((3, 2, 1)))(stacked_cnn_lstm_sae_input)\n",
    "stacked_cnn_lstm_sae_encoded_input_1 = tf.keras.layers.TimeDistributed(cnn_sae.layers[2])(stacked_cnn_lstm_sae_encoded_input_0)\n",
    "stacked_cnn_lstm_sae_encoded_input_2 = tf.keras.layers.TimeDistributed(cnn_sae.layers[3])(stacked_cnn_lstm_sae_encoded_input_1)\n",
    "stacked_cnn_lstm_sae_encoded_input_3 =  tf.keras.layers.Reshape((5, 78))(stacked_cnn_lstm_sae_encoded_input_2)\n",
    "stacked_cnn_lstm_sae_e2 = compose(lstm_sae.layers[5:7], stacked_cnn_lstm_sae_encoded_input_3)\n",
    "stacked_cnn_lstm_sae_fcnn_1 = tf.keras.layers.Dense(64, activation=activation)(stacked_cnn_lstm_sae_e2)\n",
    "stacked_cnn_lstm_sae_fcnn_2 = tf.keras.layers.Dense(24, activation=activation)(stacked_cnn_lstm_sae_fcnn_1)\n",
    "stacked_cnn_lstm_sae_output = tf.keras.layers.Dense(4, activation='softmax')(stacked_cnn_lstm_sae_fcnn_2)\n",
    "stacked_cnn_lstm_sae = tf.keras.Model(stacked_cnn_lstm_sae_input,\n",
    "                                     stacked_cnn_lstm_sae_output,\n",
    "                                     name='stacked_cnn_lstm_sae')\n",
    "stacked_cnn_lstm_sae.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23881fd9-96c2-4814-87e2-46fc937e9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_cnn_lstm_sae.fit(lstm_train_data, lstm_train_labels, epochs=100, batch_size=32,\n",
    "                  callbacks=[k.callbacks.ModelCheckpoint(filepath='stacked_cnn_lstm_sae.{epoch:02d}.keras'),\n",
    "                             k.callbacks.EarlyStopping(monitor='loss')])\n",
    "stacked_cnn_lstm_sae_scores = stacked_cnn_lstm_sae.evaluate(lstm_test_data, lstm_test_labels)\n",
    "print(\"\\n%s: %.2f%%\" % (stacked_cnn_lstm_sae.metrics_names[1], stacked_cnn_lstm_sae_scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c4c79-b3d7-485f-b5ba-244037d39b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
