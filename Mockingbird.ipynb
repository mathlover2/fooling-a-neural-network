{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154e184-7e7e-4472-a62d-bd7e4a3fe679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off Tensorflow warnings\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import random\n",
    "\n",
    "tf.keras.utils.disable_interactive_logging()\n",
    "\n",
    "## Helper functions\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as imp:  # Overwrites any existing file.\n",
    "        res = pickle.load(imp)\n",
    "    return res\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Functions to manipulate traces\n",
    "\n",
    "def add_new_burst(trace, index, sizes, interval):\n",
    "    l = len(sizes)\n",
    "    s = sum(sizes)\n",
    "    new_row = np.array([l, s, 100.0*l, 800.0*s, interval, s/l])\n",
    "    res = trace.insert(trace, index, new_row, axis=0)\n",
    "    return np.stack(res)\n",
    "\n",
    "def add_to_burst(trace, index, sizes, interval):\n",
    "    old_row = trace[index]\n",
    "    old_l, old_s = old_row[0], old_row[1]\n",
    "    old_i = old_row[4]\n",
    "    new_l = old_l + len(sizes)\n",
    "    new_s = old_s + sum(sizes)\n",
    "    new_i = (old_i*(old_l-1) + len(sizes)*interval)/(old_l+len(sizes)-1)\n",
    "    new_row =  np.array([new_l, new_s, 100*new_l, 800*new_s, new_i, new_s/new_l])\n",
    "    trace[index] = new_row\n",
    "    return trace\n",
    "\n",
    "# Default function for the purposes of this project\n",
    "\n",
    "def add_to_bursts(trace, b_j):\n",
    "    new_trace = []\n",
    "    for index in range(trace.shape[1]):\n",
    "        old_row = trace[0][index]\n",
    "        old_l, old_s = old_row[0], old_row[1]\n",
    "        old_i = old_row[4]\n",
    "        new_l = old_l + b_j[index]\n",
    "        new_s = old_s*(new_l/old_l)\n",
    "        new_i = (old_i*(old_l-1) + b_j[index]*old_i)/(new_l-1) if new_l != 1 else 0\n",
    "        new_r = new_s/new_l if new_l != 0 else 0\n",
    "        new_row = tf.convert_to_tensor([new_l, new_s, 100*new_l, 800*new_s, new_i, new_s/new_l], dtype=tf.float32)\n",
    "        new_trace.append(new_row)\n",
    "    return tf.reshape(tf.stack(new_trace), (1, 5, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974cbb3-6e6b-4fcf-94c2-7ba899d15879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mockingbird(source_trace, model, targets, **kwargs):\n",
    "    number_of_time_series = source_trace.shape[0]\n",
    "    res = []\n",
    "    for idx in range(number_of_time_series):\n",
    "        new_targets = [targ[idx].reshape(1, 5, 6) for targ in targets if targ.shape[0] > idx]\n",
    "        new_source = source_trace[idx].reshape(1, 5, 6)\n",
    "        res.append(_mockingbird(new_source, model, new_targets, **kwargs).reshape(5, 6))\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def _mockingbird(source_trace, model, targets, *,\n",
    "                alpha=0.05, d=20, threshold_c=0.3,\n",
    "                threshold_d=1, max_n=100,\n",
    "                p=5):\n",
    "    \n",
    "    I_s = source_trace\n",
    "    S = targets\n",
    "    \n",
    "    # Main procedure\n",
    "    Y_s = predict(model, source_trace)[0]\n",
    "    P_s = random.sample(S, p)\n",
    "    I_t = argmin(P_s, lambda I: metric(I_s, I))\n",
    "    I_s_new = np.copy(I_s)\n",
    "    for i in range(max_n):\n",
    "        gradient = compute_gradient(I_s_new, I_t).numpy()\n",
    "        ind = np.where(gradient >= 0)\n",
    "        I_s_new[0][ind] = np.dot((1 + alpha * gradient[ind]), I_s_new[0][ind])\n",
    "        Y_s_new, P = predict(model, I_s_new, confidence=Y_s)\n",
    "        if Y_s_new != Y_s and P < threshold_c:\n",
    "            break\n",
    "        \n",
    "        if (i%d == 0 and Y_s_new == Y_s \n",
    "           and metric(I_s_new, I_s) < threshold_d):\n",
    "            P_s = random.sample(S, p)\n",
    "            I_t = argmin(P_s, lambda I: metric(I_s, I))\n",
    "    return I_s_new\n",
    "\n",
    "def argmin(S, f):\n",
    "    minval = None; argm = None\n",
    "    for s in S:\n",
    "        if argm is None or f(s) < minval:\n",
    "            minval = f(s)\n",
    "            argm = s\n",
    "    return argm\n",
    "\n",
    "def predict(model, sample, confidence=None):\n",
    "    scaled_sample = scaler.transform(sample.reshape(5, 6)).reshape(1, 5, 6)\n",
    "    p = model.predict(scaled_sample)\n",
    "    res = p.argmax(axis=1)\n",
    "    if confidence != None:\n",
    "        return res, p[0][confidence]\n",
    "    return res\n",
    "\n",
    "def compute_gradient(I1, I2):\n",
    "    b_j = tf.constant([0.0, 0.0, 0.0, 0.0, 0.0], dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(b_j)\n",
    "        dist = -metric(add_to_bursts(I1, b_j), I2)\n",
    "    gradient = tape.gradient(dist, b_j)\n",
    "    return gradient\n",
    "    \n",
    "def metric(a, b):\n",
    "    return tf.norm(a-b)\n",
    "\n",
    "def make_result_sane(inp):\n",
    "    result = np.copy(inp)\n",
    "    result[0] = np.ceil(inp[0])\n",
    "    result[1] = np.ceil(inp[1])\n",
    "    result[2] = 100*result[0]\n",
    "    result[3] = 8*result[1]*100\n",
    "    result[4] = inp[4] if result[0] > 1 else 0\n",
    "    result[5] = result[1]/result[0] if result[0] != 0 else 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d065b20-8db5-49fe-859b-61bf22f58602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset for Mockingbird algorithm\n",
    "\n",
    "data_path = \"Data/\"\n",
    "browse_data_path = data_path + \"browse/\"\n",
    "chat_data_path = data_path + \"chat/\"\n",
    "mail_data_path = data_path + \"mail/\"\n",
    "p2p_data_path =  data_path + \"p2p/\"\n",
    "scaler = load_object('scaler.pkl')\n",
    "\n",
    "datasets = dict()\n",
    "samples = dict()\n",
    "sample_data = dict()\n",
    "concatenated_sample_data = dict()\n",
    "sample_labels = dict()\n",
    "\n",
    "\n",
    "for (index, path) in  enumerate([browse_data_path, chat_data_path,\n",
    "                      mail_data_path, p2p_data_path]):\n",
    "    datasets[path] = list(glob.glob(path + \"*.pcap\"))\n",
    "    samples[path] = random.sample(datasets[path],\n",
    "                                  len(datasets[path])//10)\n",
    "    sample_data[path] = []\n",
    "    for sample_path in samples[path]:\n",
    "        sample_data[path].append(np.loadtxt(sample_path, delimiter=',').reshape((-1,6)))\n",
    "    concatenated_sample_data[path] = np.concatenate(sample_data[path])\n",
    "    \n",
    "total_data = np.concatenate([concatenated_sample_data[path] for path in sample_data])\n",
    "scaler = StandardScaler().fit(total_data)\n",
    "scaled_data = dict()\n",
    "for data_type in sample_data:\n",
    "    scaled_data[data_type] = scaler.transform(concatenated_sample_data[data_type])\n",
    "\n",
    "for (index, path) in  enumerate([browse_data_path, chat_data_path,\n",
    "                      mail_data_path, p2p_data_path]):\n",
    "    if scaled_data[path].shape[0]%5 == 0:\n",
    "        scaled_data[path] = np.array_split(scaled_data[path], scaled_data[path].shape[0]//5)\n",
    "    else:\n",
    "        scaled_data[path] = np.array_split(scaled_data[path][:-(scaled_data[path].shape[0]%5)], scaled_data[path].shape[0]//5)\n",
    "        \n",
    "    scaled_data[path] = np.stack(scaled_data[path])\n",
    "    sample_labels[path] = np.tile(np.eye(4)[index], (scaled_data[path].shape[0], 1))\n",
    "\n",
    "train_data = np.concatenate([scaled_data[path] for path in scaled_data])\n",
    "train_labels = np.concatenate([sample_labels[path] for path in sample_labels])\n",
    "\n",
    "save_object(sample_data, 'mockingbird_sample_data.pkl')\n",
    "save_object(sample_labels, 'mockingbird_sample_labels.pkl')\n",
    "save_object(train_data, 'mockingbird_train_data.pkl')\n",
    "save_object(train_labels, 'mockingbird_train_labels.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb92aa-a5df-4e6e-9b52-8c458fbb7c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = load_object('mockingbird_train_data.pkl')\n",
    "train_labels = load_object('mockingbird_train_labels.pkl')\n",
    "\n",
    "# Create LSTM detector for Mockingbird algorithm.\n",
    "\n",
    "activation = k.activations.relu\n",
    "\n",
    "mockingbird_detector_input = tf.keras.layers.Input(shape=(5, 6))\n",
    "mockingbird_detector_e_1 = tf.keras.layers.LSTM(16, return_sequences=True)(mockingbird_detector_input)\n",
    "mockingbird_detector_e_2 = tf.keras.layers.LSTM(136, return_sequences=False)(mockingbird_detector_e_1)\n",
    "mockingbird_detector_d_repeat = tf.keras.layers.RepeatVector(5)(mockingbird_detector_e_2)\n",
    "mockingbird_detector_d_1 = tf.keras.layers.LSTM(136, return_sequences=True)(mockingbird_detector_d_repeat)\n",
    "mockingbird_detector_d_2 = tf.keras.layers.LSTM(16, return_sequences=True)(mockingbird_detector_d_1)\n",
    "mockingbird_detector_output = k.layers.Dense(6, activation='sigmoid')(mockingbird_detector_d_2)\n",
    "mockingbird_detector_sae_fcnn_1 = tf.keras.layers.Dense(64, activation=activation)(mockingbird_detector_e_2)\n",
    "mockingbird_detector_sae_fcnn_2 = tf.keras.layers.Dense(24, activation=activation)(mockingbird_detector_sae_fcnn_1)\n",
    "mockingbird_detector_sae_output = tf.keras.layers.Dense(4, activation='softmax')(mockingbird_detector_sae_fcnn_2)\n",
    "\n",
    "mockingbird_detector_sae = tf.keras.Model(inputs=mockingbird_detector_input, outputs=mockingbird_detector_output, name='mockingbird_detector')\n",
    "mockingbird_detector_sae.compile(loss='mse', optimizer='adam')\n",
    "mockingbird_detector_sae_encoder = k.Model(mockingbird_detector_input, mockingbird_detector_e_2)\n",
    "\n",
    "mockingbird_detector_sae_full = k.Model(inputs=mockingbird_detector_input, outputs=mockingbird_detector_sae_output, name='mockingbird_detector_full')\n",
    "mockingbird_detector_sae_full.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Train \n",
    "\n",
    "mockingbird_detector_sae.fit(train_data, train_data, epochs=10, batch_size=16,\n",
    "                             callbacks=[k.callbacks.ModelCheckpoint(filepath='mockingbird_detector.{epoch:02d}.keras'),\n",
    "                             k.callbacks.EarlyStopping(monitor='loss')])\n",
    "\n",
    "for layer in mockingbird_detector_sae.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "mockingbird_detector_sae_full.fit(train_data, train_labels, epochs=100, batch_size=16,\n",
    "                                   callbacks=[k.callbacks.ModelCheckpoint(filepath='mockingbird_detector_full.{epoch:02d}.keras'),\n",
    "                                   k.callbacks.EarlyStopping(monitor='loss')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99697e90-ea23-4358-8d8e-b86d77b1879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "mockingbird_detector_sae_full = tf.keras.saving.load_model('mockingbird_detector_full.38.keras')\n",
    "stacked_cnn_lstm_sae_full = tf.keras.saving.load_model('stacked_cnn_lstm_sae.53.keras')\n",
    "scaler = load_object('scaler.pkl')\n",
    "\n",
    "# Set up sources\n",
    "\n",
    "data_path = \"Data/\"\n",
    "browse_data_path = data_path + \"browse/\"\n",
    "chat_data_path = data_path + \"chat/\"\n",
    "mail_data_path = data_path + \"mail/\"\n",
    "p2p_data_path =  data_path + \"p2p/\"\n",
    "\n",
    "sample_data = load_object('mockingbird_sample_data.pkl')\n",
    "sample_labels = load_object('mockingbird_sample_labels.pkl')\n",
    "train_data = load_object('mockingbird_train_data.pkl')\n",
    "train_labels = load_object('mockingbird_train_labels.pkl')\n",
    "\n",
    "# Create sliced samples\n",
    "def create_lstm_sample(sample):\n",
    "    if sample.shape[0] % 5 == 0:\n",
    "        s = sample\n",
    "    else:\n",
    "        s = sample[:-(sample.shape[0]%5)]\n",
    "    return np.stack(np.array_split(s, sample.shape[0]//5), axis=0)\n",
    "\n",
    "sliced_sample_data = dict()\n",
    "\n",
    "for path in sample_data:\n",
    "    sliced_sample_data[path] = []\n",
    "    for sample in sample_data[path]:\n",
    "        if sample.shape[0] >= 15:\n",
    "            sliced = create_lstm_sample(sample)\n",
    "            sliced_sample_data[path].append(sliced)\n",
    "\n",
    "sources = dict()\n",
    "for path in [browse_data_path, chat_data_path, p2p_data_path]:\n",
    "    sources[path] = create_lstm_sample(random.choice([p for p in sample_data[path] if 50 >= p.shape[0] >= 15]))\n",
    "\n",
    "\n",
    "# Set up targeted pools\n",
    "\n",
    "targeted_pools = dict()\n",
    "for path in [browse_data_path, chat_data_path,\n",
    "            mail_data_path, p2p_data_path]:\n",
    "    targeted_pools[path] = sliced_sample_data[path]\n",
    "\n",
    "\n",
    "# Set up untargeted pools:\n",
    "\n",
    "untargeted_pools = dict()\n",
    "for path in [browse_data_path, chat_data_path,\n",
    "            mail_data_path, p2p_data_path]:\n",
    "    untargeted_pools[path] = [j for p in [browse_data_path, chat_data_path,mail_data_path, p2p_data_path]\n",
    "        for j in targeted_pools[p]]\n",
    "\n",
    "# Run algorithm for each pair and record the results in the logfile\n",
    "    \n",
    "with open('mockingbird.log', \"a\") as logfile:\n",
    "    print(\"Untargeted:\\n\", file=logfile)\n",
    "    logfile.flush()\n",
    "    for path in [browse_data_path, chat_data_path, p2p_data_path]:\n",
    "        print(\"Source folder:\", path, file=logfile)\n",
    "        source, pool = sources[path], untargeted_pools[path]\n",
    "        print(\"Source trace:\", source, file=logfile)\n",
    "        logfile.flush()\n",
    "        adv = mockingbird(source, mockingbird_detector_sae_full, pool)\n",
    "        print(\"Adversarial raw:\", adv, file=logfile)\n",
    "        logfile.flush()\n",
    "        sane_adv = np.apply_along_axis(make_result_sane, -1, adv)\n",
    "        print(\"Adversarial sane:\", sane_adv, file=logfile)\n",
    "        logfile.flush()\n",
    "        res = stacked_cnn_lstm_sae_full.predict(sane_adv).argmax(axis=-1)\n",
    "        vals, counts = np.unique(res, return_counts=True)\n",
    "        mr = vals[np.argmax(counts)]\n",
    "        print(\"Predictions:\", res, file=logfile)\n",
    "        print(\"Majority Prediction:\", mr, file=logfile)\n",
    "        logfile.flush()\n",
    "        \n",
    "    print(\"Targeted:\\n\", file=logfile)\n",
    "    for (path1, path2) in permutations([browse_data_path, chat_data_path, p2p_data_path], 2):\n",
    "        print(\"Source folder:\", path1, file=logfile)\n",
    "        print(\"Target folder:\", path2, file=logfile)\n",
    "        logfile.flush()\n",
    "        source, pool = sources[path1], targeted_pools[path2]\n",
    "        print(\"Source trace:\", source, file=logfile)\n",
    "        logfile.flush()\n",
    "        adv = mockingbird(source, mockingbird_detector_sae_full, pool)\n",
    "        print(\"Adversarial raw:\", adv, file=logfile)\n",
    "        logfile.flush()\n",
    "        sane_adv = np.apply_along_axis(make_result_sane, -1, adv)\n",
    "        print(\"Adversarial sane:\", sane_adv, file=logfile)\n",
    "        logfile.flush()\n",
    "        res = stacked_cnn_lstm_sae_full.predict(sane_adv).argmax(axis=-1)\n",
    "        vals, counts = np.unique(res, return_counts=True)\n",
    "        mr = vals[np.argmax(counts)]\n",
    "        print(\"Predictions:\", res, file=logfile)\n",
    "        print(\"Majority Prediction:\", mr, file=logfile)\n",
    "        logfile.flush()\n",
    "    print(\"END LOG\", file=logfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
