{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dbc2aa-b4af-4e21-a23c-799ff7e6e62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn off Tensorflow warnings\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "## Key imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "tf.keras.utils.disable_interactive_logging()\n",
    "\n",
    "## Helper functions\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as imp:  # Overwrites any existing file.\n",
    "        res = pickle.load(imp)\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf2619f-a67e-45bc-b134-8c75c3b4fcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load objects \n",
    "\n",
    "lstm_test_data = load_object('lstm_test_data.pkl')\n",
    "lstm_test_labels = load_object('lstm_test_labels.pkl')\n",
    "lstm_test_data = np.concatenate([lstm_test_data[d] for d in ['browse','chat','mail','p2p']])\n",
    "lstm_test_labels = np.concatenate([lstm_test_labels[d] for d in ['browse','chat','mail','p2p']])\n",
    "scaler = load_object('scaler.pkl')\n",
    "\n",
    "stacked_cnn_lstm_sae = tf.keras.saving.load_model('stacked_cnn_lstm_sae.77.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0567485e-13d2-4463-879c-8544511b3846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "data_path = \"Data/\"\n",
    "browse_data_path = data_path + \"browse/\"\n",
    "chat_data_path = data_path + \"chat/\"\n",
    "mail_data_path = data_path + \"mail/\"\n",
    "p2p_data_path =  data_path + \"p2p/\"\n",
    "data_paths = [browse_data_path, chat_data_path, mail_data_path, p2p_data_path]\n",
    "\n",
    "datasets = dict()\n",
    "sample_data = dict()\n",
    "sample_labels = dict()\n",
    "scaled_sample_data = dict()\n",
    "test_sample_data = dict()\n",
    "\n",
    "for (index, path) in  enumerate(data_paths):\n",
    "    datasets[path] = list(glob.glob(path + \"*.pcap\"))\n",
    "    sample_data[path] = []\n",
    "\n",
    "    for sample_path in datasets[path]:\n",
    "        s = np.loadtxt(sample_path, delimiter=',').reshape((-1,6))\n",
    "        if s.shape[0] >= 15:\n",
    "            sample_data[path].append(s)\n",
    "\n",
    "\n",
    "for (idx, path) in enumerate(data_paths):\n",
    "    acc = []\n",
    "    samples = sample_data[path]\n",
    "    scaled_sample_data[path] = []\n",
    "    for sample in samples:\n",
    "        scaled_sample = scaler.transform(sample)\n",
    "        if scaled_sample.shape[0] % 5 == 0:\n",
    "            s = scaled_sample\n",
    "        else:\n",
    "            s =  scaled_sample[:-(scaled_sample.shape[0]%5)]\n",
    "        new_sample = np.stack(np.array_split(s, sample.shape[0]//5))\n",
    "        res = tf.argmax(stacked_cnn_lstm_sae(new_sample)).numpy()\n",
    "        vals, counts = np.unique(res, return_counts=True)\n",
    "        mr = vals[np.argmax(counts)]\n",
    "        if mr == idx:\n",
    "            acc.append(tf.convert_to_tensor(new_sample))\n",
    "    scaled_sample_data[path] = acc\n",
    "\n",
    "for path in data_paths:\n",
    "    test_sample_data[path] = random.sample(scaled_sample_data[path], 5)\n",
    "    \n",
    "for (idx, path) in enumerate(data_paths):\n",
    "    sample_labels[path] = []\n",
    "    for sample in test_sample_data[path]:\n",
    "        sample_labels[path].append(np.tile(np.eye(4)[idx], (sample.shape[0], 1)))\n",
    "\n",
    "\n",
    "# p2p_sample = np.array_split(scaled_p2p_sample_data[:-(scaled_p2p_sample_data.shape[0]%5)],\n",
    "#                         scaled_p2p_sample_data.shape[0]//5)\n",
    "# p2p_sample = tf.convert_to_tensor(p2p_sample)\n",
    "\n",
    "# browse_sample = np.array_split(scaled_browse_sample_data[:-(scaled_browse_sample_data.shape[0]%5)],\n",
    "#                         scaled_browse_sample_data.shape[0]//5)\n",
    "# browse_sample = tf.convert_to_tensor(browse_sample)\n",
    "\n",
    "\n",
    "# mail_sample = np.array_split(scaled_mail_sample_data[:-(scaled_mail_sample_data.shape[0]%5)],\n",
    "#                         scaled_mail_sample_data.shape[0]//5)\n",
    "# mail_sample = tf.convert_to_tensor(mail_sample)\n",
    "\n",
    "\n",
    "# p2p_label = np.tile(np.array([0.0, 0.0, 0.0, 1.0]),  (p2p_sample_data.shape[0]//5, 1))\n",
    "# browse_label = np.tile(np.array([1.0, 0.0, 0.0, 0.0]),  (browse_sample_data.shape[0]//5, 1))\n",
    "# mail_label = np.tile(np.array([0.0, 0.0, 1.0, 0.0]),  (mail_sample_data.shape[0]//5, 1))\n",
    "\n",
    "# # Code below based on https://www.tensorflow.org/tutorials/generative/adversarial_fgsm\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def create_adversarial_pattern(sample, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(sample)\n",
    "        prediction = stacked_cnn_lstm_sae(sample)\n",
    "        loss = loss_object(label, prediction)\n",
    "        \n",
    "    # Get the gradients of the loss w.r.t to the input image.\n",
    "    gradient = tape.gradient(loss, sample)\n",
    "    # Get the sign of the gradients to create the perturbation\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    return signed_grad\n",
    "\n",
    "# This function is the \"reverse\" of scikit-learns standard scaler, mapping \n",
    "# scaled values back to the unscaled input space\n",
    "\n",
    "def reverse_scaler(scaler, scaled_input):\n",
    "    res = scaled_input\n",
    "    if scaler.with_std:\n",
    "        res *= scaler.scale_\n",
    "    if scaler.with_mean:\n",
    "        res += scaler.mean_\n",
    "    return res\n",
    "\n",
    "def make_result_sane(result):\n",
    "    result[0] = np.ceil(result[0])\n",
    "    result[1] = np.ceil(result[1])\n",
    "    result[2] = 100*result[0]\n",
    "    result[3] = 8*result[1]*100\n",
    "    result[4] = result[4] if result[0] > 1 else 0\n",
    "    result[5] = result[1]/result[0] if result[0] != 0 else 0\n",
    "    return result\n",
    "\n",
    "def scale(scaler):\n",
    "    def _(inp):\n",
    "        res = inp\n",
    "        if scaler.with_mean:\n",
    "            res -= scaler.mean_\n",
    "        if scaler.with_std:\n",
    "            res /= scaler.scale_\n",
    "        return res\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8eee23-05f3-4cf7-8bc1-c089536513f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.0,0.01,0.05,0.1,0.2,0.25]\n",
    "\n",
    "with open('fgsm.log', 'a') as logfile:\n",
    "    for path in data_paths:\n",
    "        print(path + \":\\n\\n\\n\", file=logfile)\n",
    "        for (idx, sample) in enumerate(test_sample_data[path]):\n",
    "            print(\"Scaled test sample:\", test_sample_data[path][idx], file=logfile)\n",
    "            print(\"Label:\", sample_labels[path][idx], file=logfile)\n",
    "            perturbations = create_adversarial_pattern(test_sample_data[path][idx], sample_labels[path][idx])\n",
    "            print(\"Adversarial perturbation:\", perturbations, \"\\n\\n\", file=logfile)\n",
    "            for eps in epsilons:\n",
    "                print(\"Epsilon =\", eps, file=logfile)\n",
    "                adversarial = test_sample_data[path][idx] + eps*perturbations\n",
    "                print(\"Adversarial sample:\", adversarial, file=logfile)\n",
    "                unscaled_adversarial = reverse_scaler(scaler, adversarial).numpy()\n",
    "                print(\"Unscaled adversarial sample:\", unscaled_adversarial, file=logfile)\n",
    "                sane_adversarial = np.apply_along_axis(make_result_sane, 2, unscaled_adversarial)\n",
    "                print(\"Unscaled adversarial sample made_sane:\", sane_adversarial, file=logfile)\n",
    "                scaled_adversarial = np.apply_along_axis(scale(scaler), 2, sane_adversarial)\n",
    "                print(\"Scaled adversarial sample made sane:\", scaled_adversarial, file=logfile)\n",
    "                results = stacked_cnn_lstm_sae.predict(scaled_adversarial).argmax(axis=-1)\n",
    "                print(\"Prediction results:\", results, file=logfile)\n",
    "                vals, counts = np.unique(results, return_counts=True)\n",
    "                for (v, c) in zip(vals, counts):\n",
    "                    print(\"Class\", v, \"occurs\", c, \"times\",  file=logfile)\n",
    "                print(file=logfile)\n",
    "    print(\"END LOG\", file=logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71e33e-c48c-4244-9ff9-75a276aac96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
